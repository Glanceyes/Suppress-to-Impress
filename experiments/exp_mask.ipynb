{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from config import RunConfig\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats, ndimage\n",
    "from skimage.feature import peak_local_max\n",
    "from pycocotools import mask\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "from utils import vis_utils\n",
    "from diffusers import DDIMScheduler, DDIMInverseScheduler\n",
    "from pipeline_scribble_guide import ScribbleGuidePipeline, AttentionStore\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_image(segmentations, image_to_file, segm_idx, image_idx):\n",
    "  segm = segmentations[\"annotations\"][segm_idx][\"segmentation\"]\n",
    "  image = segmentations[\"annotations\"][image_idx][\"image_id\"]\n",
    "  image = image_to_file[image]\n",
    "  image = np.array(Image.open(image).convert(\"RGB\"))\n",
    "  h, w, c = image.shape\n",
    "  rles = mask.frPyObjects(segm, h, w)\n",
    "  \n",
    "  if type(rles) is dict:\n",
    "    rles = [rles]\n",
    "  \n",
    "  rle = mask.merge(rles)\n",
    "  segm = mask.decode(rle)\n",
    "  segm = segm * 255\n",
    "\n",
    "  segm, image = Image.fromarray(segm), Image.fromarray(image)\n",
    "  return segm, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_images(init_image, mask_image):\n",
    "    # Ensure both images are the same size\n",
    "    assert init_image.size == mask_image.size, \"Images must be the same size!\"\n",
    "    \n",
    "    # Convert both images to RGBA to work with transparency\n",
    "    init_image = init_image.convert(\"RGBA\")\n",
    "    mask_image = mask_image.convert(\"RGBA\")\n",
    "    \n",
    "    # Create a new image with the same size and RGBA mode for the result\n",
    "    blended_image = Image.new(\"RGBA\", init_image.size)\n",
    "    \n",
    "    # Iterate over each pixel\n",
    "    for y in range(init_image.height):\n",
    "        for x in range(init_image.width):\n",
    "            mask_pixel = mask_image.getpixel((x, y))\n",
    "            init_pixel = init_image.getpixel((x, y))\n",
    "            \n",
    "            # If the mask_pixel is white, keep the init_pixel\n",
    "            if mask_pixel[:3] == (255, 255, 255):\n",
    "                blended_image.putpixel((x, y), init_pixel)\n",
    "            else:\n",
    "                # Otherwise, set the pixel to the mask_pixel\n",
    "                blended_image.putpixel((x, y), mask_pixel)\n",
    "    \n",
    "    return blended_image.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../datasets\"\n",
    "shape_prompts = json.load(open(os.path.join(dataset_dir, \"shape_prompts\", \"val.json\")))\n",
    "\n",
    "annotations = shape_prompts['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_id_list = [ annotation['category_id']  for annotation in annotations ]\n",
    "annotation_id_set_list = list(set(annotation_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotation_id_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = [ category for category in shape_prompts['categories'] if category['id'] in annotation_id_set_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_to_name = {}\n",
    "category_name_to_id = {}\n",
    "\n",
    "for category in category_list:\n",
    "    category_id_to_name[category['id']] = category['name']\n",
    "    category_name_to_id[category['name']] = category['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_id_to_name)\n",
    "print(category_name_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count = defaultdict(int)\n",
    "for annotation in annotations:\n",
    "    category_id = annotation['category_id']\n",
    "    category_count[category_id_to_name[category_id]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_file = {image[\"id\"]: image[\"coco_url\"].replace(\"http://images.cocodataset.org\", dataset_dir) for image in shape_prompts[\"images\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in range(len(annotations)):\n",
    "#     annotation = annotations[index]\n",
    "#     category_name = category_id_to_name[annotation['category_id']]\n",
    "    \n",
    "#     mask_image, init_image = get_segmentation_image(shape_prompts, image_to_file, index, index)\n",
    "#     mask_image, init_image = mask_image.resize((512, 512)), init_image.resize((512, 512))\n",
    "    \n",
    "#     blended_image = blend_images(init_image, mask_image)\n",
    "    \n",
    "#     save_category_image_path = f'./dataset/category/{category_name}'\n",
    "#     save_masked_image_path = f'./dataset/masked'\n",
    "    \n",
    "#     if not os.path.exists(save_category_image_path):\n",
    "#         os.makedirs(save_category_image_path)\n",
    "    \n",
    "#     if not os.path.exists(save_masked_image_path):\n",
    "#         os.makedirs(save_masked_image_path)\n",
    "    \n",
    "#     blended_image.save(f'{save_category_image_path}/{index}.jpg')\n",
    "#     blended_image.save(f'{save_masked_image_path}/{category_name}_{index}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scribble Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner_id = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(captioner_id)\n",
    "model = BlipForConditionalGeneration.from_pretrained(captioner_id, \n",
    "                                                    #  torch_dtype=torch.float16, \n",
    "                                                     low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_model_ckpt = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "stable = ScribbleGuidePipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    caption_generator=model,\n",
    "    caption_processor=processor,\n",
    "    safety_checker=None,\n",
    "    # torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "tokenizer = stable.tokenizer\n",
    "stable.scheduler = DDIMScheduler.from_config(stable.scheduler.config)\n",
    "stable.inverse_scheduler = DDIMInverseScheduler.from_config(stable.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_prompt(prompt: List[str],\n",
    "                  token_masks: Union[\n",
    "                    List[torch.Tensor],\n",
    "                    List[Image.Image],\n",
    "                    List[np.ndarray]  \n",
    "                  ],\n",
    "                  model: ScribbleGuidePipeline,\n",
    "                  controller: AttentionStore,\n",
    "                  token_indices: List[int],\n",
    "                  seed: int,\n",
    "                  output_path: str,\n",
    "                  generator: torch.Generator,\n",
    "                  config: RunConfig,\n",
    "                  latents: torch.FloatTensor = None,\n",
    "                  ) -> Image.Image:\n",
    "    outputs = model(prompt=prompt,\n",
    "                    token_masks=token_masks,\n",
    "                    attention_store=controller,\n",
    "                    indices_list=token_indices,\n",
    "                    attention_resolution=config.attention_res,\n",
    "                    guidance_scale=config.guidance_scale,\n",
    "                    latents=latents,\n",
    "                    generator=generator,\n",
    "                    seed=seed,\n",
    "                    output_path=output_path,\n",
    "                    num_inference_steps=config.num_inference_steps,\n",
    "                    run_standard=config.run_standard,\n",
    "                    scale_factor=config.scale_factor,\n",
    "                    scale_range=config.scale_range,\n",
    "                  )\n",
    "    image = outputs.images[0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_display(prompts: List[str],\n",
    "                    token_masks: Union[\n",
    "                        List[torch.Tensor],\n",
    "                        List[Image.Image],\n",
    "                        List[np.ndarray]  \n",
    "                    ],\n",
    "                    controller: AttentionStore,\n",
    "                    indices_to_alter: List[int],\n",
    "                    seed: int,\n",
    "                    output_path: str,\n",
    "                    generator: torch.Generator,\n",
    "                    latents: torch.FloatTensor = None,\n",
    "                    run_standard: bool = False,\n",
    "                    scale_factor: int = 10,\n",
    "                    display_output: bool = False):\n",
    "    config = RunConfig(prompt=prompts[0],\n",
    "                       run_standard=run_standard,\n",
    "                       scale_factor=scale_factor)\n",
    "    image = run_on_prompt(model=stable,\n",
    "                          token_masks=token_masks,\n",
    "                          prompt=prompts,\n",
    "                          latents=latents,\n",
    "                          controller=controller,\n",
    "                          output_path=output_path,\n",
    "                          token_indices=indices_to_alter,\n",
    "                          seed=seed,\n",
    "                          generator=generator,\n",
    "                          config=config)\n",
    "    if display_output:\n",
    "        display(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(annotations)):\n",
    "    annotation = annotations[index]\n",
    "    category_name = category_id_to_name[annotation['category_id']]\n",
    "    \n",
    "    mask_image, _ = get_segmentation_image(shape_prompts, image_to_file, index, index)\n",
    "    mask_image = mask_image.resize((512, 512))\n",
    "    \n",
    "    prompt = f\"a photography of {'an' if category_name[0] in ['a', 'e', 'i', 'o', 'u'] else 'a'} {category_name}\"\n",
    "    \n",
    "    prompts = [prompt]\n",
    "    token_indices = [5]\n",
    "    token_masks = [mask_image]\n",
    "    \n",
    "    seed = 21\n",
    "    latents = None\n",
    "    \n",
    "    controller = AttentionStore()\n",
    "    \n",
    "    generator = torch.Generator('cuda').manual_seed(seed)\n",
    "    \n",
    "    display(mask_image)\n",
    "    image = run_and_display(prompts=prompts,\n",
    "                            # image=image,\n",
    "                            token_masks=token_masks,\n",
    "                            controller=controller,\n",
    "                            latents=latents,\n",
    "                            indices_to_alter=token_indices,\n",
    "                            generator=generator,\n",
    "                            seed=seed,\n",
    "                            output_path=f\"runs/{index}\",\n",
    "                            run_standard=False,\n",
    "                            display_output=True)\n",
    "    vis_utils.show_cross_attention(attention_store=controller,\n",
    "                                   prompt=prompt,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   res=16,\n",
    "                                   from_where=(\"up\", \"down\", \"mid\"),\n",
    "                                   indices_to_alter=token_indices,\n",
    "                                   global_attention=False,\n",
    "                                   orig_image=image)\n",
    "    vis_utils.show_self_attention(attention_store=controller,\n",
    "                                    res=16,\n",
    "                                    from_where=(\"up\", \"down\", \"mid\"),\n",
    "                                )\n",
    "    \n",
    "    # image = run_and_display(prompts=prompts,\n",
    "    #                         # image=image,\n",
    "    #                         token_masks=token_masks,\n",
    "    #                         controller=controller,\n",
    "    #                         latents=latents,\n",
    "    #                         indices_to_alter=token_indices,\n",
    "    #                         generator=generator,\n",
    "    #                         seed=seed,\n",
    "    #                         run_standard=True,\n",
    "    #                         display_output=True)\n",
    "    # vis_utils.show_cross_attention(attention_store=controller,\n",
    "    #                                prompt=prompt,\n",
    "    #                                tokenizer=tokenizer,\n",
    "    #                                res=16,\n",
    "    #                                from_where=(\"up\", \"down\", \"mid\"),\n",
    "    #                                indices_to_alter=token_indices,\n",
    "    #                                global_attention=False,\n",
    "    #                                orig_image=image)\n",
    "    # vis_utils.show_self_attention(attention_store=controller,\n",
    "    #                                 res=16,\n",
    "    #                                 from_where=(\"up\", \"down\", \"mid\"),\n",
    "    #                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attend-excite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
